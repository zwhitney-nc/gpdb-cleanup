# -*- coding: utf-8 -*-
"""LowfConfPMM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TcG3UYmeerXIenPcOducjZcjmyC56tv3
"""

# this won't work if on local machine
"""from google.colab import drive
drive.mount('/content/drive')"""

!source venv/bin/activate

!pip install git+https://github.com/openai/whisper.git

!pwd

#hosted runtime only
#%ls drive/Shareddrives/Data\ Department/Data\ Assets/Phonetics

!pip install pandas

# for hosted runtime only
"""from os import chdir
chdir('/content/drive/Shareddrives/Data Department/Data Assets/Phonetics')"""

from numpy import argmin, isnan, any, all, intersect1d, arange, random, array, where, logical_not, isnan, concatenate, append
from collections import Counter
from itertools import compress, islice
import pandas as pd

#load in csv to check and print the first few rows to check
df = pd.read_csv("GPDB-cleanup-prepped.csv")
print(df.shape)
# df = pd.read_csv("Uncategorized_badsources.csv")
# df = df.head(50) #limiting to just the top 50 names for a test

# the full dataset is gigantic, so I'll start with a more manageable chunk, saving the original
#df_orig = df.copy()
df = df_orig.iloc[200:int(len(df_orig) * .05)]
df.reset_index(drop=True, inplace=True)
print(df.shape)

# save the relevant columns as separate variables
names = df['name_gpdb']
recs = df['gpdb_url']
date = df["created_at"]

# names = df["name"]
# recs = df["url_hedb"]
print(names.head())
# print(recs.head())
# print(names.tail())

# pull saved column data into a dict of the form {"name INDEX", (recording url, date recorded)}
data = {}
for ind, name in enumerate(names):
  rec = recs[ind]
  dt = date[ind]
  if type(name)==float: #nan name
    name = "InvalidName "+str(ind)
    data[name] = (rec, dt)
    # data[name] = (rec)
    continue
  if name in data:
    name = name + str(ind)
  if type(rec)==float or rec.isnumeric(): #nan or numerical rec link
    rec = "InvalidRec "+str(ind)
    data[name] = (rec, dt)
    # data[name] = (rec)
    continue
  if name.isnumeric():
    name = "InvalidName "+str(ind)
    data[name] = (rec, dt)
    # data[name] = (rec)
    continue
  if not rec.startswith("https://production-processed-recordings"):
    name = "IncompatibleRec "+str(ind)
  data[name] = (rec, dt)
  # data[name] = (rec)
print(dict(islice(data.items(),100)))

import whisper
model = whisper.load_model("small")

!pip install textdistance
!pip install stopit
!pip install alphabet_detector

from textdistance import mra, editex, hamming
import stopit
from alphabet_detector import AlphabetDetector
from collections import deque
import numpy as np
ad = AlphabetDetector()

"""
this code is for running on a hosted runtime
!cp /content/drive/Shareddrives/Data\ Department/Data\ Assets/Phonetics/gpdb-cleanup-key.json /content/.config/
!export GOOGLE_APPLICATION_CREDENTIALS="/content/.config/gpdb-cleanup-key.json" #need to make sure you copy this from Data Assets/Phonetics
from google.oauth2 import service_account
credentials = service_account.Credentials.from_service_account_file(
    "/content/.config/gpdb-cleanup-key.json")"""

# this code is for running on a local runtime
!export GOOGLE_APPLICATION_CREDENTIALS="gpdb-cleanup-key.json" #need to actually have this file in your workding directory
!pip install google-auth

from google.oauth2 import service_account
credentials = service_account.Credentials.from_service_account_file(
    "gpdb-cleanup-key.json")


!pip install google-api-core
!pip install google-cloud-translate
!pip install idna
import six
# !ls /usr/local/lib/python3.10/dist-packages/idna-3.4.dist-info/METADATA'
from google.cloud import translate_v2 as translate

!gcloud auth application-default login --impersonate-service-account gpdb-cleanup@gpdb-cleanup.iam.gserviceaccount.com
#you need to be impersonating the service account, not logged into your own
# !gcloud auth login zac@name-coach.com
# !gcloud auth print-access-token --impersonate-service-account=gpdb-cleanup@gpdb-cleanup.iam.gserviceaccount.com
!gcloud config set project gpdb-cleanup

#!gcloud auth application-default set-quota-project gpdb-cleanup

def pre_transliterate(word):
  translate_client = translate.Client()
  if isinstance(word, six.binary_type):
      text = text.decode("utf-8")
  print("Google Translating: ", word)
  result = translate_client.translate(word)
  return result["translatedText"]

namekeys = data.keys()
# namekeys = np.array([k for k in namekeys if data[k][1].startswith("2020")])
namekeys = np.array(list(namekeys))

sil = deque() #for each recording in data, contains whether the recording is silent or not
targets = deque() #for each recording in data, contains whether the recording matches the target name
confs = deque() #to contain confidence of predictions

!pip install ffmpeg

"""ONE OF THESE PRINTS OUT A 200% CONFIDENCE SCORE; benjaminashmead"""

#adding in timer to get a sense of how fast this works on local
import timeit
start = timeit.default_timer()

for name in namekeys:
  print(name)
  #rec = data[name]
  rec = data[name][0]
  if rec.startswith("Inval"):
    sil.append("N/A")
    targets.append("N/A")
    confs.append("N/A")
    continue
  elif name.startswith("Incomp"): # incompatible recording - youtube, twitter, etc
    sil.append("N/A")
    targets.append("N/A")
    confs.append("N/A")
    continue
  try:
    audio = whisper.load_audio(rec)
  except RuntimeError:
    sil.append("yes")
    targets.append("no")
    confs.append(100)
    continue
  with stopit.ThreadingTimeout(12) as context_manager1:
    # try:
    first = whisper.transcribe(model, audio)
    # except AssertionError:
    #   continue
  if context_manager1.state == context_manager1.EXECUTED:
    transcription = first["text"]
    empty = ["", " "]
    if transcription in empty:
      sil.append("yes")
      targets.append("no")
      confs.append(100)
      continue
    else:
      sil.append("no")
      transcription = transcription.strip(" ")
      if not ad.is_latin(transcription[-1]):
        transcription = pre_transliterate(transcription)
      names = transcription.split(" ")
      sim = {n:editex.similarity(name, n) for n in names}
      simsort = sorted(sim, key=sim.get)
      tgt = simsort[-1]
      finaldist = editex.distance(name, tgt.lower())
      if finaldist <= 5:
        # print(finaldist)
        targets.append("yes")
        confs.append(round(finaldist*16.67))
        continue
      transcription = [t.strip(",.?") for t in names if t.lower() != name]
      catsim = mra.similarity(name, "".join(transcription))
      if catsim > 2:
        targets.append("yes")
        confs.append(round(catsim*33.33))
      else:
        targets.append("no")
        confs.append(100)
  elif context_manager1.state == context_manager1.TIMED_OUT: # Did code timeout?
      sil.append("yes")
      targets.append("no")
      confs.append(100)
      continue

end = timeit.default_timer()
timeElapsed = end - start

print(f'took {timeElapsed} seconds for {len(namekeys)} names, {len(namekeys)/timeElapsed} names per second')

# print(list(zip(targets, namekeys[:200]))[:len(sil)], sil, confs)
print(list(zip(sil, namekeys))[:len(sil)], targets, confs)

#package up results and write to new CSV

## print(len(is_sil.values), len(df))
#dfnew = df.loc[df["created_at"].str.startswith("2020")][:200]
dfnew = df.copy()
index_values = dfnew.index
dfnew["is_silent"] = pd.Series(np.array(sil), index=index_values)
dfnew["is_target"] = pd.Series(np.array(targets), index=index_values)
dfnew["target_confidence_%"] = pd.Series(np.array(confs), index=index_values)

# df["is_silent"] = pd.Series(np.array(sil))
# df["is_target"] = pd.Series(np.array(targets))
# df["target_confidence_%"] = pd.Series(np.array(confs))

dfnew.to_csv('gpdb_cleanup_TEST_first201-184088.csv')
# df.to_csv("Fernando_Categorized_badsources.csv")